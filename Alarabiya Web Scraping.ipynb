{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de6507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading https://www.alarabiya.net/...\n",
      "Parsing...\n",
      "Reading healines...\n",
      "Writing the results...\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "def today():\n",
    "    \n",
    "    # returns today's date\n",
    "    today = str(date.today())\n",
    "\n",
    "    # to match the Al-Arabiya's formatting, we'll change ' - ' to ' / '\n",
    "    today = today.replace('-', '/') \n",
    "\n",
    "    return today\n",
    "\n",
    "def read_webpage(url):\n",
    "    # makes a https request to the url\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    \n",
    "    html_page = urlopen(req).read()\n",
    "\n",
    "    # returns the html page\n",
    "    return html_page\n",
    "\n",
    "def parse_webpage(html_page):\n",
    "    # converts the website into a 'Beautoful Soup' object so that we can scrape it\n",
    "    parsed = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "    return parsed\n",
    "\n",
    "def read_headlines(webpage, article_date):\n",
    "    headlines = list()\n",
    "\n",
    "    for link in webpage.find_all('a'):\n",
    "        str_link = str(\n",
    "            link.get('href')    # get the article url\n",
    "        )\n",
    "        if article_date in str_link:    # select links that are from today\n",
    "            str_link_splited = str_link.split('/')\n",
    "\n",
    "            headline = str_link_splited[-1]     # isolate the article name\n",
    "\n",
    "            headline_clean = headline.replace('-', ' ')     # make it readable\n",
    "\n",
    "            headlines.append(headline_clean)\n",
    "\n",
    "    unique_headlines = set(headlines)   #remove duplicates\n",
    "\n",
    "    return unique_headlines\n",
    "\n",
    "def write_in_file(text, file_name):\n",
    "    with open(file_name, 'w+', encoding='utf-8') as output:\n",
    "        for row in text:\n",
    "            output.write(row + '\\n')    # writes and adds in a new line after each article\n",
    "\n",
    "def run():\n",
    "    website_address = 'https://www.alarabiya.net/'\n",
    "    output_file_name = 'alarabiya_headlines.txt'\n",
    "\n",
    "    print(f'Reading {website_address}...')\n",
    "    html_page = read_webpage(website_address)\n",
    "\n",
    "    print(f'Parsing...')\n",
    "    parsed_page = parse_webpage(html_page)\n",
    "\n",
    "    article_date = today()\n",
    "\n",
    "    print(f'Reading healines...')\n",
    "    headlines = read_headlines(parsed_page, article_date)\n",
    "\n",
    "    print(f'Writing the results...')\n",
    "    write_in_file(headlines, output_file_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n",
    "\n",
    "    end = input(\n",
    "        'Type any key and press Enter to exit.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafc95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862ab0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
