{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2df6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Scraper\n",
      "The following websites are available to scrape: Al-Arabiya, RT Arabic, Sky News Arabic, Al-Youm Al-Sabbah, and Al-Hurra        \n",
      "Type in a number from the list below:\n",
      "1 -> Scrape All        \n",
      "2 -> Scrape Al-Arabiya        \n",
      "3 -> Scrape RT Arabic        \n",
      "4 -> Scrape Sky News Arabic        \n",
      "5 -> Scrape Al-Youm Al-Sabbah        \n",
      "6 -> Scrape Al-Hurra\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a number and hit the Enter key:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening RT Arabic\n",
      "Finding Articles\n",
      "121 articles found\n",
      "Reading headlines. This will take a while.\n",
      "Found 44 articles from today. Writing them into the rt_headlines.txt file.\n",
      "Opening Al-Youm Al-Sabeh\n",
      "Finding Headlines\n",
      "Found 123 articles from today. Writing them into the youm7_headlines.txt file.\n",
      "Opening Al-Youm Al-Hurra\n",
      "Finding Headlines\n",
      "Found 14 articles from today. Writing them into the alhurra_headlines.txt file.\n",
      "Successfully Done!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, datetime\n",
    "\n",
    "\n",
    "def scrape_alarabiya():\n",
    "    pass\n",
    "\n",
    "def scrape_rt():\n",
    "    url = 'https://arabic.rt.com/'\n",
    "\n",
    "    print('Opening RT Arabic')\n",
    "    request = Request(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "\n",
    "    html_page = urlopen(request).read()\n",
    "\n",
    "    parsed_page = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "    website_links = list()\n",
    "\n",
    "    print('Finding Articles')\n",
    "    for link in parsed_page.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        href = str(href)\n",
    "        if '%' in href:\n",
    "            website_links.append(href)\n",
    "    \n",
    "    website_links = set(website_links)\n",
    "    print(f'{len(website_links)} articles found')\n",
    "\n",
    "    article_slug = 'https://arabic.rt.com'\n",
    "    article_links = list()\n",
    "\n",
    "    for link in website_links:\n",
    "        article_link = article_slug + link\n",
    "        article_links.append(article_link)\n",
    "    \n",
    "    today = date.today()\n",
    "    today = str(today)\n",
    "    today = today.split('-')\n",
    "    today = today[::-1]\n",
    "    today = '.'.join(today)\n",
    "    \n",
    "    print('Reading headlines. This will take a while.')\n",
    "    headlines = list()\n",
    "    for link in article_links:\n",
    "        sub_request = Request(link, headers={'User-Agent':'Mozilla/5.0'})\n",
    "        try:\n",
    "            sub_html_page = urlopen(sub_request).read()\n",
    "        except:\n",
    "            continue\n",
    "        sub_parsed_page = BeautifulSoup(sub_html_page, 'html.parser')\n",
    "\n",
    "        date_spans = sub_parsed_page.find_all('span', {'class': 'date'})\n",
    "        header_spans = sub_parsed_page.find_all('h1', {'class': 'heading'})\n",
    "\n",
    "        if len(date_spans) < 1 or len(header_spans) < 1:\n",
    "            continue\n",
    "\n",
    "        article_date = date_spans[0].text\n",
    "        \n",
    "        if today in article_date:\n",
    "            article_headline = header_spans[0].text\n",
    "            headlines.append(\n",
    "                article_headline\n",
    "            )\n",
    "    print(f'Found {len(headlines)} articles from today. Writing them into the rt_headlines.txt file.')\n",
    "\n",
    "    output_file_name = 'rt_headlines.txt'\n",
    "    with open(output_file_name, 'w+', encoding='utf-8') as output_file:\n",
    "        for article_headline in headlines:\n",
    "            output_file.write(article_headline + '\\n')\n",
    "        \n",
    "    \n",
    "def scrape_youm7():\n",
    "    url = 'https://www.youm7.com/'\n",
    "\n",
    "    print('Opening Al-Youm Al-Sabeh')\n",
    "    request = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    html_page = urlopen(request).read()\n",
    "\n",
    "    parsed_page = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "    today = date.today()\n",
    "    today = str(today)\n",
    "    today = today.split('-')\n",
    "    if today[1][0] == '0':\n",
    "        month = today[1].replace('0', '')\n",
    "        today = today[0] + '/' + month + '/' + today[2]\n",
    "    else:\n",
    "        today = today[0] + '/' + today[1] + '/' + today[2]\n",
    "\n",
    "    print('Finding Headlines')\n",
    "    headlines = list()\n",
    "\n",
    "    for link in parsed_page.find_all('a'):\n",
    "        str_link = link.get('href')\n",
    "        if today in str_link:\n",
    "            headline = str_link.split('/')\n",
    "\n",
    "            headline = headline[-2]\n",
    "\n",
    "            headline = headline.replace('-', ' ')\n",
    "\n",
    "            headlines.append(headline)\n",
    "    \n",
    "    headlines = set(headlines)\n",
    "\n",
    "    print(f'Found {len(headlines)} articles from today. Writing them into the youm7_headlines.txt file.')\n",
    "\n",
    "    output_file_name = 'youm7_headlines.txt'\n",
    "    with open(output_file_name, 'w+', encoding='utf-8') as output_file:\n",
    "        for article_headline in headlines:\n",
    "            output_file.write(article_headline + '\\n')\n",
    "\n",
    "\n",
    "def scrape_alhurra():\n",
    "    url = 'https://www.alhurra.com/'\n",
    "\n",
    "    print('Opening Al-Youm Al-Hurra')\n",
    "    request = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    html_page = urlopen(request).read()\n",
    "\n",
    "    parsed_page = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "    today = date.today()\n",
    "    today = str(today)\n",
    "    today = today.replace('-', '/')\n",
    "\n",
    "    print('Finding Headlines')\n",
    "    headlines = list()\n",
    "\n",
    "    for link in parsed_page.find_all('a'):\n",
    "        str_link = link.get('href')\n",
    "        if today in str_link:\n",
    "            headline = link.findNext('span').text\n",
    "\n",
    "            headlines.append(headline)\n",
    "    \n",
    "    headlines = set(headlines)\n",
    "\n",
    "    print(f'Found {len(headlines)} articles from today. Writing them into the alhurra_headlines.txt file.')\n",
    "\n",
    "    output_file_name = 'alhurra_headlines.txt'\n",
    "    with open(output_file_name, 'w+', encoding='utf-8') as output_file:\n",
    "        for article_headline in headlines:\n",
    "            output_file.write(article_headline + '\\n')\n",
    "\n",
    "\n",
    "def menu():\n",
    "    menu_prompt_welcome = 'Welcome to Scraper'\n",
    "    menu_prompt_options = 'The following websites are available to scrape: Al-Arabiya, RT Arabic, Sky News Arabic, Al-Youm Al-Sabbah, and Al-Hurra\\\n",
    "        \\nType in a number from the list below:\\n1 -> Scrape All\\\n",
    "        \\n2 -> Scrape Al-Arabiya\\\n",
    "        \\n3 -> Scrape RT Arabic\\\n",
    "        \\n4 -> Scrape Sky News Arabic\\\n",
    "        \\n5 -> Scrape Al-Youm Al-Sabbah\\\n",
    "        \\n6 -> Scrape Al-Hurra'\n",
    "    \n",
    "    print(menu_prompt_welcome)\n",
    "    print(menu_prompt_options)\n",
    "\n",
    "\n",
    "def run():\n",
    "    user_input_prompt = 'Enter a number and hit the Enter key: '\n",
    "\n",
    "    while True:\n",
    "        menu()\n",
    "\n",
    "        success = False\n",
    "\n",
    "        while not success:\n",
    "            user_input = input(user_input_prompt)\n",
    "            if user_input == '1':\n",
    "                scrape_all()\n",
    "                print('Successfully Done!')\n",
    "            elif user_input == '2':\n",
    "                scrape_alarabiya()\n",
    "                print('Successfully Done!')\n",
    "            elif user_input == '3':\n",
    "                scrape_rt()\n",
    "                print('Successfully Done!')\n",
    "            elif user_input == '4':\n",
    "                scrape_youm7()\n",
    "                print('Successfully Done!')\n",
    "            elif user_input == '5':\n",
    "                scrape_alhurra()\n",
    "                print('Successfully Done!')\n",
    "            else:\n",
    "                print('Input incorrect....')\n",
    "\n",
    "\n",
    "def scrape_all():\n",
    "    scrape_alarabiya()\n",
    "    scrape_rt()\n",
    "    scrape_youm7()\n",
    "    scrape_alhurra()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4822b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef86d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a12fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b94a8-e618-4316-b0c2-4c1c28b7d86c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
